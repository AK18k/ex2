{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AK18k/ex2/blob/main/ex2_028064558_027244037_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning - Ex #2\n",
        "Avi Keinan 028064558\n",
        "Ofer Ballin 027244037\n",
        "\n",
        "This notebook implements LSTM and GRU with and without dropouts"
      ],
      "metadata": {
        "id": "MtUKmt-upo9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext google.colab.data_table\n",
        "import pdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNmJlmS3Dmkb",
        "outputId": "c91f8cb2-9c83-46c3-c691-c74dfb5aa1ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The google.colab.data_table extension is already loaded. To reload it, use:\n",
            "  %reload_ext google.colab.data_table\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AK18k/ex2"
      ],
      "metadata": {
        "id": "fd4aj_pCh7VP",
        "outputId": "29b60301-257b-45c9-aa10-58f80a4efcce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ex2'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 65 (delta 18), reused 38 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (65/65), 3.83 MiB | 5.63 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "DATA_PATH = '/content/ex2/data/ptb'\n",
        "PATH = '/content/ex2'\n",
        "os.chdir('/content/ex2')\n",
        "!ls"
      ],
      "metadata": {
        "id": "IRPe9rMApsmL",
        "outputId": "a96c7b0f-bcfe-4bd5-b210-43bef907e202",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adabound.py\t\t\t\t      finetune.py\t model.py\n",
            "corpus.77afd5b9c5825ef44c47fcd18f4839dd.data  generate.py\t pointer.py\n",
            "data\t\t\t\t\t      getdata.sh\t README.md\n",
            "data.py\t\t\t\t\t      LICENSE\t\t splitcross.py\n",
            "embed_regularize.py\t\t\t      locked_dropout.py  utils.py\n",
            "ex2_028064558_027244037_new.ipynb\t      main.py\t\t weight_drop.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/ex2')"
      ],
      "metadata": {
        "id": "_BC5Z_88jAF6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from adabound import AdaBound\n",
        "\n",
        "import data\n",
        "import model\n",
        "\n",
        "from utils import batchify, get_batch, repackage_hidden"
      ],
      "metadata": {
        "id": "gcHclAz2n2R8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='PyTorch PennTreeBank RNN/LSTM Language Model')\n",
        "parser.add_argument('--data', type=str, default='data/ptb/',\n",
        "                    help='location of the data corpus')\n",
        "parser.add_argument('--model', type=str, default='LSTM',\n",
        "                    help='type of recurrent net (LSTM, QRNN, GRU)')\n",
        "parser.add_argument('--emsize', type=int, default=1500,\n",
        "                    help='size of word embeddings')\n",
        "parser.add_argument('--nhid', type=int, default=200,\n",
        "                    help='number of hidden units per layer')\n",
        "parser.add_argument('--nlayers', type=int, default=3,\n",
        "                    help='number of layers')\n",
        "parser.add_argument('--lr', type=float, default=0.1,\n",
        "                    help='initial learning rate')\n",
        "# parser.add_argument('--clip', type=float, default=0.25,\n",
        "#                     help='gradient clipping')\n",
        "parser.add_argument('--epochs', type=int, default=200,\n",
        "                    help='upper epoch limit')\n",
        "parser.add_argument('--batch_size', type=int, default=20, metavar='N',\n",
        "                    help='batch size')\n",
        "parser.add_argument('--bptt', type=int, default=70,\n",
        "                    help='sequence length')\n",
        "parser.add_argument('--dropout', type=float, default=0,\n",
        "                    help='dropout applied to layers (0 = no dropout)')\n",
        "parser.add_argument('--dropouth', type=float, default=0,\n",
        "                    help='dropout for rnn layers (0 = no dropout)')\n",
        "parser.add_argument('--dropouti', type=float, default=0,\n",
        "                    help='dropout for input embedding layers (0 = no dropout)')\n",
        "parser.add_argument('--dropoute', type=float, default=0,\n",
        "                    help='dropout to remove words from embedding layer (0 = no dropout)')\n",
        "parser.add_argument('--wdrop', type=float, default=0,\n",
        "                    help='amount of weight dropout to apply to the RNN hidden to hidden matrix')\n",
        "parser.add_argument('--seed', type=int, default=1111,\n",
        "                    help='random seed')\n",
        "parser.add_argument('--nonmono', type=int, default=1111,\n",
        "                    help='random seed')\n",
        "parser.add_argument('--cuda', action='store_false',\n",
        "                    help='use CUDA')\n",
        "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
        "                    help='report interval')\n",
        "randomhash = ''.join(str(time.time()).split('.'))\n",
        "parser.add_argument('--save', type=str,  default=randomhash+'.pt',\n",
        "                    help='path to save the final model')\n",
        "parser.add_argument('--alpha', type=float, default=2,\n",
        "                    help='alpha L2 regularization on RNN activation (alpha = 0 means no regularization)')\n",
        "parser.add_argument('--beta', type=float, default=1,\n",
        "                    help='beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)')\n",
        "parser.add_argument('--wdecay', type=float, default=5e-4,\n",
        "                    help='weight decay applied to all weights')\n",
        "parser.add_argument('--resume', type=str,  default='',\n",
        "                    help='path of model to resume')\n",
        "parser.add_argument('--when', nargs=\"+\", type=int, default=[-1],\n",
        "                    help='When (which epochs) to divide the learning rate by 10 - accepts multiple')\n",
        "parser.add_argument('--optim', default='sgd', type=str, help='optimizer',\n",
        "                    choices=['sgd', 'adagrad', 'adam', 'amsgrad', 'adabound', 'amsbound'])\n",
        "parser.add_argument('--momentum', default=0.9, type=float, help='momentum term')\n",
        "parser.add_argument('--beta1', default=0.9, type=float, help='Adam coefficients beta_1')\n",
        "parser.add_argument('--beta2', default=0.999, type=float, help='Adam coefficients beta_2')\n",
        "parser.add_argument('--final_lr', default=0.1, type=float,\n",
        "                    help='final learning rate of AdaBound')\n",
        "parser.add_argument('--gamma', default=1e-3, type=float,)\n",
        "parser.add_argument('--ita',default=1e-2, type=float)\n",
        "parser.add_argument('--weight_decay', default=5e-4, type=float,\n",
        "                    help='weight decay for optimizers')\n",
        "#args = parser.parse_args()\n",
        "args, unknown = parser.parse_known_args() #change args assignment to accomodate Jupiter execution (instead of command line)\n",
        "args.tied = True\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "print(args.cuda)\n",
        "if torch.cuda.is_available():\n",
        "    print('cuda is available')\n",
        "    if not args.cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "    else:\n",
        "        torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DjfO_4nqY7k",
        "outputId": "ca59ccb3-d757-4b81-8f00-1f98c7d0c263"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "cuda is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "def model_save(fn):\n",
        "    with open(fn, 'wb') as f:\n",
        "        torch.save([model, criterion, optimizer], f)\n",
        "\n",
        "def model_load(fn):\n",
        "    global model, criterion, optimizer\n",
        "    with open(fn, 'rb') as f:\n",
        "        model, criterion, optimizer = torch.load(f)\n",
        "\n",
        "import os\n",
        "import hashlib\n",
        "fn = 'corpus.{}.data'.format(hashlib.md5(args.data.encode()).hexdigest())\n",
        "if os.path.exists(fn):\n",
        "    print('Loading cached dataset...')\n",
        "    corpus = torch.load(fn)\n",
        "else:\n",
        "    print('Producing dataset...')\n",
        "    corpus = data.Corpus(args.data)\n",
        "    torch.save(corpus, fn)\n",
        "\n",
        "eval_batch_size = 10\n",
        "\n",
        "test_batch_size = 1\n",
        "train_data = batchify(corpus.train, args.batch_size, args)\n",
        "val_data = batchify(corpus.valid, eval_batch_size, args)\n",
        "test_data = batchify(corpus.test, test_batch_size, args)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g08GWZBlXT-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "795879aa-31b6-448b-b396-1ffb6d04d13d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.shape)\n",
        "print(args.batch_size)\n",
        "print(train_data[0][2].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjjSjsQpq5Ej",
        "outputId": "bb21d487-e868-4ecb-bc37-b46c2f951443"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([46479, 20])\n",
            "20\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "from splitcross import SplitCrossEntropyLoss\n",
        "criterion = None\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.dropouth, args.dropouti, args.dropoute, args.wdrop, args.tied)\n",
        "###\n",
        "if args.resume:\n",
        "    print('Resuming model ...')\n",
        "    model_load(args.resume)\n",
        "    optimizer.param_groups[0]['lr'] = args.lr\n",
        "    model.dropouti, model.dropouth, model.dropout, args.dropoute = args.dropouti, args.dropouth, args.dropout, args.dropoute\n",
        "    if args.wdrop:\n",
        "        from weight_drop import WeightDrop\n",
        "        for rnn in model.rnns:\n",
        "            if type(rnn) == WeightDrop: rnn.dropout = args.wdrop\n",
        "            elif rnn.zoneout > 0: rnn.zoneout = args.wdrop\n",
        "###\n",
        "if not criterion:\n",
        "    splits = []\n",
        "    if ntokens > 500000:\n",
        "        # One Billion\n",
        "        # This produces fairly even matrix mults for the buckets:\n",
        "        # 0: 11723136, 1: 10854630, 2: 11270961, 3: 11219422\n",
        "        splits = [4200, 35000, 180000]\n",
        "    elif ntokens > 75000:\n",
        "        # WikiText-103\n",
        "        splits = [2800, 20000, 76000]\n",
        "    print('Using', splits)\n",
        "    criterion = SplitCrossEntropyLoss(args.emsize, splits=splits, verbose=False)\n",
        "###\n",
        "if args.cuda:\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "###\n",
        "params = list(model.parameters()) + list(criterion.parameters())\n",
        "total_params = sum(x.size()[0] * x.size()[1] if len(x.size()) > 1 else x.size()[0] for x in params if x.size())\n",
        "print('Args:', args)\n",
        "print('Model total parameters:', total_params)"
      ],
      "metadata": {
        "id": "P-lv_fRwXbch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab8a2c3-820c-479d-c7f1-ff1a2fcae8ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LSTM(1500, 200), LSTM(200, 200), LSTM(200, 1500)]\n",
            "Using []\n",
            "Args: Namespace(data='data/ptb/', model='LSTM', emsize=1500, nhid=200, nlayers=3, lr=0.1, epochs=200, batch_size=20, bptt=70, dropout=0, dropouth=0, dropouti=0, dropoute=0, wdrop=0, seed=1111, nonmono=1111, cuda=True, log_interval=200, save='16848270302836006.pt', alpha=2, beta=1, wdecay=0.0005, resume='', when=[-1], optim='sgd', momentum=0.9, beta1=0.9, beta2=0.999, final_lr=0.1, gamma=0.001, ita=0.01, weight_decay=0.0005, tied=True)\n",
            "Model total parameters: 26905200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CdX7JOJSnwdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "def evaluate(data_source, batch_size=10):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    if args.model == 'QRNN': model.reset()\n",
        "    total_loss = 0\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    for i in range(0, data_source.size(0) - 1, args.bptt):\n",
        "        data, targets = get_batch(data_source, i, args, evaluation=True)       \n",
        "        breakpoint()\n",
        "        output, hidden = model(data, hidden)\n",
        "        total_loss += len(data) * criterion(model.decoder.weight, model.decoder.bias, output, targets).data\n",
        "        breakpoint()\n",
        "        hidden = repackage_hidden(hidden)\n",
        "    return total_loss.item() / len(data_source)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    if args.model == 'QRNN': model.reset()\n",
        "    total_loss = 0\n",
        "    # start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(args.batch_size)\n",
        "    batch, i = 0, 0\n",
        "    while i < train_data.size(0) - 1 - 1:\n",
        "        bptt = args.bptt if np.random.random() < 0.95 else args.bptt / 2.\n",
        "        # Prevent excessively small or negative sequence lengths\n",
        "        seq_len = max(5, int(np.random.normal(bptt, 5)))\n",
        "        # There's a very small chance that it could select a very long sequence length resulting in OOM\n",
        "        # seq_len = min(seq_len, args.bptt + 10)\n",
        "\n",
        "        lr2 = optimizer.param_groups[0]['lr']\n",
        "        optimizer.param_groups[0]['lr'] = lr2 * seq_len / args.bptt\n",
        "        model.train()\n",
        "        data, targets = get_batch(train_data, i, args, seq_len=seq_len)\n",
        "\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        breakpoint()\n",
        "        output, hidden, rnn_hs, dropped_rnn_hs = model(data, hidden, return_h=True)\n",
        "        raw_loss = criterion(model.decoder.weight, model.decoder.bias, output, targets)\n",
        "\n",
        "        loss = raw_loss\n",
        "        # Activiation Regularization\n",
        "        if args.alpha: loss = loss + sum(args.alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n",
        "        # Temporal Activation Regularization (slowness)\n",
        "        if args.beta: loss = loss + sum(args.beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        # if args.clip: torch.nn.utils.clip_grad_norm_(params, args.clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += raw_loss.data\n",
        "        optimizer.param_groups[0]['lr'] = lr2\n",
        "        # if batch % args.log_interval == 0 and batch > 0:\n",
        "        # total_loss = 0\n",
        "            # start_time = time.time()\n",
        "        ###\n",
        "        batch += 1\n",
        "        i += seq_len\n",
        "\n",
        "    train_loss = total_loss.item() / batch\n",
        "    # elapsed = time.time() - start_time\n",
        "    print('train_loss  {:5.2f} | train_ppl {:8.2f} | train_bpc {:8.3f}'.format(\n",
        "        train_loss, math.exp(train_loss), train_loss / math.log(2)))\n",
        "# Loop over epochs.\n",
        "lr = args.lr\n",
        "best_val_loss = []\n",
        "stored_loss = 100000000\n",
        "\n",
        "def create_optimizer(args, model_params):\n",
        "    if args.optim == 'sgd':\n",
        "        return optim.SGD(model_params, args.lr, momentum=args.momentum,\n",
        "                         weight_decay=args.weight_decay)\n",
        "    elif args.optim == 'adagrad':\n",
        "        return optim.Adagrad(model_params, args.lr, weight_decay=args.weight_decay)\n",
        "    elif args.optim == 'adam':\n",
        "        return optim.Adam(model_params, args.lr, betas=(args.beta1, args.beta2),\n",
        "                          weight_decay=args.weight_decay)\n",
        "    elif args.optim == 'amsgrad':\n",
        "        return optim.Adam(model_params, args.lr, betas=(args.beta1, args.beta2),\n",
        "                          weight_decay=args.weight_decay, amsgrad=True)\n",
        "    elif args.optim == 'adabound':\n",
        "        return AdaBound(model_params, args.lr, betas=(args.beta1, args.beta2),\n",
        "                        final_lr=args.final_lr, gamma=args.gamma,\n",
        "                        weight_decay=args.weight_decay)\n",
        "    else:\n",
        "        assert args.optim == 'amsbound'\n",
        "        return AdaBound(model_params, args.lr, betas=(args.beta1, args.beta2),\n",
        "                        final_lr=args.final_lr, gamma=args.gamma,\n",
        "                        weight_decay=args.weight_decay, amsbound=True)\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    optimizer = create_optimizer(args, model.parameters())\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        print(\"epoch:\" + str(epoch))\n",
        "        breakpoint()\n",
        "        train()\n",
        "        if 't0' in optimizer.param_groups[0]:\n",
        "            tmp = {}\n",
        "            for prm in model.parameters():\n",
        "                tmp[prm] = prm.data.clone()\n",
        "                prm.data = optimizer.state[prm]['ax'].clone()\n",
        "\n",
        "            test_loss = evaluate(test_data, test_batch_size)\n",
        "            print('test loss {:5.2f} | test ppl {:8.2f} | test bpc {:8.3f}'.format(\n",
        "                test_loss, math.exp(test_loss), test_loss / math.log(2)))\n",
        "            val_loss2 = evaluate(val_data)\n",
        "            print('-' * 89)\n",
        "            print(' valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f} | valid bpc {:8.3f}'.format(\n",
        "                    val_loss2, math.exp(val_loss2), val_loss2 / math.log(2)))\n",
        "            print('-' * 89)\n",
        "\n",
        "            if val_loss2 < stored_loss:\n",
        "                model_save(args.save)\n",
        "                print('Saving Averaged!')\n",
        "                stored_loss = val_loss2\n",
        "\n",
        "            for prm in model.parameters():\n",
        "                prm.data = tmp[prm].clone()\n",
        "\n",
        "        else:\n",
        "            val_loss = evaluate(val_data, eval_batch_size)\n",
        "            print('-' * 89)\n",
        "            print(' valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f} | valid bpc {:8.3f}'.format(\n",
        "              val_loss, math.exp(val_loss), val_loss / math.log(2)))\n",
        "            print('-' * 89)\n",
        "\n",
        "            if val_loss < stored_loss:\n",
        "                model_save(args.save)\n",
        "                print('Saving model (new best validation)')\n",
        "                stored_loss = val_loss\n",
        "\n",
        "            if args.optim == 'sgd' and 't0' not in optimizer.param_groups[0] and (len(best_val_loss)>args.nonmono and val_loss > min(best_val_loss[:-args.nonmono])):\n",
        "                print('Switching to ASGD')\n",
        "                optimizer = torch.optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n",
        "\n",
        "            if epoch in args.when:\n",
        "                print('Saving model before learning rate decreased')\n",
        "                model_save('{}.e{}'.format(args.save, epoch))\n",
        "                print('Dividing learning rate by 10')\n",
        "                optimizer.param_groups[0]['lr'] /= 10.\n",
        "\n",
        "            best_val_loss.append(val_loss)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "model_load(args.save)\n",
        "\n",
        "# Run on test data."
      ],
      "metadata": {
        "id": "3aVdQQiEXj_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e7c6086-b08a-44b9-f015-8b0e11f0f4c1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch:1\n",
            "> \u001b[0;32m<ipython-input-9-a5a2d9ffb6fa>\u001b[0m(109)\u001b[0;36m<cell line: 103>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    107 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    108 \u001b[0;31m        \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 109 \u001b[0;31m        \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    110 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0;34m't0'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    111 \u001b[0;31m            \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 347, in set_continue\n",
            "    sys.settrace(None)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m<ipython-input-9-a5a2d9ffb6fa>\u001b[0m(48)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     46 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     47 \u001b[0;31m        \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 48 \u001b[0;31m        \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_hs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropped_rnn_hs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_h\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     49 \u001b[0;31m        \u001b[0mraw_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     50 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> c\n",
            "> \u001b[0;32m<ipython-input-9-a5a2d9ffb6fa>\u001b[0m(48)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     46 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     47 \u001b[0;31m        \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 48 \u001b[0;31m        \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_hs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropped_rnn_hs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_h\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     49 \u001b[0;31m        \u001b[0mraw_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     50 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> c\n",
            "> \u001b[0;32m<ipython-input-9-a5a2d9ffb6fa>\u001b[0m(48)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     46 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     47 \u001b[0;31m        \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 48 \u001b[0;31m        \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_hs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropped_rnn_hs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_h\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     49 \u001b[0;31m        \u001b[0mraw_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     50 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbmOIwkUeDO3",
        "outputId": "e1d121e6-7be7-4691-b403-c86dcbd683ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}