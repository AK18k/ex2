{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AK18k/ex2/blob/main/ex2_028064558_027244037_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning - Ex #2\n",
        "Avi Keinan 028064558\n",
        "Ofer Ballin 027244037\n",
        "\n",
        "This notebook implements LSTM and GRU with and without dropouts"
      ],
      "metadata": {
        "id": "MtUKmt-upo9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_PATH = '/content/drive/MyDrive/Ex2/data/ptb'\n",
        "PATH = '/content/drive/MyDrive/Ex'\n",
        "os.chdir('/content/drive/MyDrive/Ex2')\n",
        "!ls"
      ],
      "metadata": {
        "id": "IRPe9rMApsmL",
        "outputId": "b5566673-a25c-4d67-b7d8-0cbc9791c3ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cd42b6e6b1d8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mDATA_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Ex2/data/ptb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Ex'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AK18k/ex2"
      ],
      "metadata": {
        "id": "fd4aj_pCh7VP",
        "outputId": "9e1500a9-d918-4ed9-945d-48e852b33cb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ex2'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 40 (delta 1), reused 40 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (40/40), 3.81 MiB | 8.64 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/ex2')"
      ],
      "metadata": {
        "id": "_BC5Z_88jAF6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5kzb4FWgrTNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from adabound import AdaBound\n",
        "\n",
        "import data\n",
        "import model\n",
        "\n",
        "from utils import batchify, get_batch, repackage_hidden\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch PennTreeBank RNN/LSTM Language Model')\n",
        "parser.add_argument('--data', type=str, default='data/ptb/',\n",
        "                    help='location of the data corpus')\n",
        "parser.add_argument('--model', type=str, default='LSTM',\n",
        "                    help='type of recurrent net (LSTM, QRNN, GRU)')\n",
        "parser.add_argument('--emsize', type=int, default=1500,\n",
        "                    help='size of word embeddings')\n",
        "parser.add_argument('--nhid', type=int, default=200,\n",
        "                    help='number of hidden units per layer')\n",
        "parser.add_argument('--nlayers', type=int, default=3,\n",
        "                    help='number of layers')\n",
        "parser.add_argument('--lr', type=float, default=0.1,\n",
        "                    help='initial learning rate')\n",
        "# parser.add_argument('--clip', type=float, default=0.25,\n",
        "#                     help='gradient clipping')\n",
        "parser.add_argument('--epochs', type=int, default=200,\n",
        "                    help='upper epoch limit')\n",
        "parser.add_argument('--batch_size', type=int, default=20, metavar='N',\n",
        "                    help='batch size')\n",
        "parser.add_argument('--bptt', type=int, default=70,\n",
        "                    help='sequence length')\n",
        "parser.add_argument('--dropout', type=float, default=0,\n",
        "                    help='dropout applied to layers (0 = no dropout)')\n",
        "parser.add_argument('--dropouth', type=float, default=0,\n",
        "                    help='dropout for rnn layers (0 = no dropout)')\n",
        "parser.add_argument('--dropouti', type=float, default=0,\n",
        "                    help='dropout for input embedding layers (0 = no dropout)')\n",
        "parser.add_argument('--dropoute', type=float, default=0,\n",
        "                    help='dropout to remove words from embedding layer (0 = no dropout)')\n",
        "parser.add_argument('--wdrop', type=float, default=0,\n",
        "                    help='amount of weight dropout to apply to the RNN hidden to hidden matrix')\n",
        "parser.add_argument('--seed', type=int, default=1111,\n",
        "                    help='random seed')\n",
        "parser.add_argument('--nonmono', type=int, default=1111,\n",
        "                    help='random seed')\n",
        "parser.add_argument('--cuda', action='store_false',\n",
        "                    help='use CUDA')\n",
        "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
        "                    help='report interval')\n",
        "randomhash = ''.join(str(time.time()).split('.'))\n",
        "parser.add_argument('--save', type=str,  default=randomhash+'.pt',\n",
        "                    help='path to save the final model')\n",
        "parser.add_argument('--alpha', type=float, default=2,\n",
        "                    help='alpha L2 regularization on RNN activation (alpha = 0 means no regularization)')\n",
        "parser.add_argument('--beta', type=float, default=1,\n",
        "                    help='beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)')\n",
        "parser.add_argument('--wdecay', type=float, default=5e-4,\n",
        "                    help='weight decay applied to all weights')\n",
        "parser.add_argument('--resume', type=str,  default='',\n",
        "                    help='path of model to resume')\n",
        "parser.add_argument('--when', nargs=\"+\", type=int, default=[-1],\n",
        "                    help='When (which epochs) to divide the learning rate by 10 - accepts multiple')\n",
        "parser.add_argument('--optim', default='sgd', type=str, help='optimizer',\n",
        "                    choices=['sgd', 'adagrad', 'adam', 'amsgrad', 'adabound', 'amsbound'])\n",
        "parser.add_argument('--momentum', default=0.9, type=float, help='momentum term')\n",
        "parser.add_argument('--beta1', default=0.9, type=float, help='Adam coefficients beta_1')\n",
        "parser.add_argument('--beta2', default=0.999, type=float, help='Adam coefficients beta_2')\n",
        "parser.add_argument('--final_lr', default=0.1, type=float,\n",
        "                    help='final learning rate of AdaBound')\n",
        "parser.add_argument('--gamma', default=1e-3, type=float,)\n",
        "parser.add_argument('--ita',default=1e-2, type=float)\n",
        "parser.add_argument('--weight_decay', default=5e-4, type=float,\n",
        "                    help='weight decay for optimizers')\n",
        "#args = parser.parse_args()\n",
        "args, unknown = parser.parse_known_args() #change args assignment to accomodate Jupiter execution (instead of command line)\n",
        "args.tied = True\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "print(args.cuda)\n",
        "if torch.cuda.is_available():\n",
        "    if not args.cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "    else:\n",
        "        torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "6DjfO_4nqY7k",
        "outputId": "ad4acbc5-13eb-408e-dc46-7fcaedfe0cb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-07a8e739b3c8>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0madabound\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdaBound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'adabound'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "def model_save(fn):\n",
        "    with open(fn, 'wb') as f:\n",
        "        torch.save([model, criterion, optimizer], f)\n",
        "\n",
        "def model_load(fn):\n",
        "    global model, criterion, optimizer\n",
        "    with open(fn, 'rb') as f:\n",
        "        model, criterion, optimizer = torch.load(f)\n",
        "\n",
        "import os\n",
        "import hashlib\n",
        "fn = 'corpus.{}.data'.format(hashlib.md5(args.data.encode()).hexdigest())\n",
        "if os.path.exists(fn):\n",
        "    print('Loading cached dataset...')\n",
        "    corpus = torch.load(fn)\n",
        "else:\n",
        "    print('Producing dataset...')\n",
        "    corpus = data.Corpus(args.data)\n",
        "    torch.save(corpus, fn)\n",
        "\n",
        "eval_batch_size = 10\n",
        "test_batch_size = 1\n",
        "train_data = batchify(corpus.train, args.batch_size, args)\n",
        "val_data = batchify(corpus.valid, eval_batch_size, args)\n",
        "test_data = batchify(corpus.test, test_batch_size, args)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g08GWZBlXT-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c14fea3-fd5c-44e6-9819-94dc26597471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjjSjsQpq5Ej",
        "outputId": "92057ef3-62a3-4228-b82f-e57d0652d4a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   0,   93,   27,  ...,  997,  392, 4210],\n",
              "        [   1,  718,  930,  ...,   42, 5518,  467],\n",
              "        [   2,  590,   42,  ...,  507, 3034, 1496],\n",
              "        ...,\n",
              "        [ 152,  170,  392,  ...,  682, 2264, 9999],\n",
              "        [4955, 6784, 4864,  ..., 6849,   42,  119],\n",
              "        [4150,  133,   26,  ..., 6344, 3401, 1143]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "from splitcross import SplitCrossEntropyLoss\n",
        "criterion = None\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.dropouth, args.dropouti, args.dropoute, args.wdrop, args.tied)\n",
        "###\n",
        "if args.resume:\n",
        "    print('Resuming model ...')\n",
        "    model_load(args.resume)\n",
        "    optimizer.param_groups[0]['lr'] = args.lr\n",
        "    model.dropouti, model.dropouth, model.dropout, args.dropoute = args.dropouti, args.dropouth, args.dropout, args.dropoute\n",
        "    if args.wdrop:\n",
        "        from weight_drop import WeightDrop\n",
        "        for rnn in model.rnns:\n",
        "            if type(rnn) == WeightDrop: rnn.dropout = args.wdrop\n",
        "            elif rnn.zoneout > 0: rnn.zoneout = args.wdrop\n",
        "###\n",
        "if not criterion:\n",
        "    splits = []\n",
        "    if ntokens > 500000:\n",
        "        # One Billion\n",
        "        # This produces fairly even matrix mults for the buckets:\n",
        "        # 0: 11723136, 1: 10854630, 2: 11270961, 3: 11219422\n",
        "        splits = [4200, 35000, 180000]\n",
        "    elif ntokens > 75000:\n",
        "        # WikiText-103\n",
        "        splits = [2800, 20000, 76000]\n",
        "    print('Using', splits)\n",
        "    criterion = SplitCrossEntropyLoss(args.emsize, splits=splits, verbose=False)\n",
        "###\n",
        "if args.cuda:\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "###\n",
        "params = list(model.parameters()) + list(criterion.parameters())\n",
        "total_params = sum(x.size()[0] * x.size()[1] if len(x.size()) > 1 else x.size()[0] for x in params if x.size())\n",
        "print('Args:', args)\n",
        "print('Model total parameters:', total_params)"
      ],
      "metadata": {
        "id": "P-lv_fRwXbch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e7237d9-490e-4b77-bbc8-4e762dbd5688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LSTM(1500, 200), LSTM(200, 200), LSTM(200, 1500)]\n",
            "Using []\n",
            "Args: Namespace(data='data/ptb/', model='LSTM', emsize=1500, nhid=200, nlayers=3, lr=0.1, epochs=200, batch_size=20, bptt=70, dropout=0, dropouth=0, dropouti=0, dropoute=0, wdrop=0, seed=1111, nonmono=1111, cuda=True, log_interval=200, save='16844940313809652.pt', alpha=2, beta=1, wdecay=0.0005, resume='', when=[-1], optim='sgd', momentum=0.9, beta1=0.9, beta2=0.999, final_lr=0.1, gamma=0.001, ita=0.01, weight_decay=0.0005, tied=True)\n",
            "Model total parameters: 26905200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "def evaluate(data_source, batch_size=10):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    if args.model == 'QRNN': model.reset()\n",
        "    total_loss = 0\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    for i in range(0, data_source.size(0) - 1, args.bptt):\n",
        "        data, targets = get_batch(data_source, i, args, evaluation=True)\n",
        "        output, hidden = model(data, hidden)\n",
        "        total_loss += len(data) * criterion(model.decoder.weight, model.decoder.bias, output, targets).data\n",
        "        hidden = repackage_hidden(hidden)\n",
        "    return total_loss.item() / len(data_source)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    if args.model == 'QRNN': model.reset()\n",
        "    total_loss = 0\n",
        "    # start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(args.batch_size)\n",
        "    batch, i = 0, 0\n",
        "    while i < train_data.size(0) - 1 - 1:\n",
        "        bptt = args.bptt if np.random.random() < 0.95 else args.bptt / 2.\n",
        "        # Prevent excessively small or negative sequence lengths\n",
        "        seq_len = max(5, int(np.random.normal(bptt, 5)))\n",
        "        # There's a very small chance that it could select a very long sequence length resulting in OOM\n",
        "        # seq_len = min(seq_len, args.bptt + 10)\n",
        "\n",
        "        lr2 = optimizer.param_groups[0]['lr']\n",
        "        optimizer.param_groups[0]['lr'] = lr2 * seq_len / args.bptt\n",
        "        model.train()\n",
        "        data, targets = get_batch(train_data, i, args, seq_len=seq_len)\n",
        "\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output, hidden, rnn_hs, dropped_rnn_hs = model(data, hidden, return_h=True)\n",
        "        raw_loss = criterion(model.decoder.weight, model.decoder.bias, output, targets)\n",
        "\n",
        "        loss = raw_loss\n",
        "        # Activiation Regularization\n",
        "        if args.alpha: loss = loss + sum(args.alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n",
        "        # Temporal Activation Regularization (slowness)\n",
        "        if args.beta: loss = loss + sum(args.beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        # if args.clip: torch.nn.utils.clip_grad_norm_(params, args.clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += raw_loss.data\n",
        "        optimizer.param_groups[0]['lr'] = lr2\n",
        "        # if batch % args.log_interval == 0 and batch > 0:\n",
        "        # total_loss = 0\n",
        "            # start_time = time.time()\n",
        "        ###\n",
        "        batch += 1\n",
        "        i += seq_len\n",
        "\n",
        "    train_loss = total_loss.item() / batch\n",
        "    # elapsed = time.time() - start_time\n",
        "    print('train_loss  {:5.2f} | train_ppl {:8.2f} | train_bpc {:8.3f}'.format(\n",
        "        train_loss, math.exp(train_loss), train_loss / math.log(2)))\n",
        "# Loop over epochs.\n",
        "lr = args.lr\n",
        "best_val_loss = []\n",
        "stored_loss = 100000000\n",
        "\n",
        "def create_optimizer(args, model_params):\n",
        "    if args.optim == 'sgd':\n",
        "        return optim.SGD(model_params, args.lr, momentum=args.momentum,\n",
        "                         weight_decay=args.weight_decay)\n",
        "    elif args.optim == 'adagrad':\n",
        "        return optim.Adagrad(model_params, args.lr, weight_decay=args.weight_decay)\n",
        "    elif args.optim == 'adam':\n",
        "        return optim.Adam(model_params, args.lr, betas=(args.beta1, args.beta2),\n",
        "                          weight_decay=args.weight_decay)\n",
        "    elif args.optim == 'amsgrad':\n",
        "        return optim.Adam(model_params, args.lr, betas=(args.beta1, args.beta2),\n",
        "                          weight_decay=args.weight_decay, amsgrad=True)\n",
        "    elif args.optim == 'adabound':\n",
        "        return AdaBound(model_params, args.lr, betas=(args.beta1, args.beta2),\n",
        "                        final_lr=args.final_lr, gamma=args.gamma,\n",
        "                        weight_decay=args.weight_decay)\n",
        "    else:\n",
        "        assert args.optim == 'amsbound'\n",
        "        return AdaBound(model_params, args.lr, betas=(args.beta1, args.beta2),\n",
        "                        final_lr=args.final_lr, gamma=args.gamma,\n",
        "                        weight_decay=args.weight_decay, amsbound=True)\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    optimizer = create_optimizer(args, model.parameters())\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        print(\"epoch:\" + str(epoch))\n",
        "        train()\n",
        "        if 't0' in optimizer.param_groups[0]:\n",
        "            tmp = {}\n",
        "            for prm in model.parameters():\n",
        "                tmp[prm] = prm.data.clone()\n",
        "                prm.data = optimizer.state[prm]['ax'].clone()\n",
        "\n",
        "            test_loss = evaluate(test_data, test_batch_size)\n",
        "            print('test loss {:5.2f} | test ppl {:8.2f} | test bpc {:8.3f}'.format(\n",
        "                test_loss, math.exp(test_loss), test_loss / math.log(2)))\n",
        "            val_loss2 = evaluate(val_data)\n",
        "            print('-' * 89)\n",
        "            print(' valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f} | valid bpc {:8.3f}'.format(\n",
        "                    val_loss2, math.exp(val_loss2), val_loss2 / math.log(2)))\n",
        "            print('-' * 89)\n",
        "\n",
        "            if val_loss2 < stored_loss:\n",
        "                model_save(args.save)\n",
        "                print('Saving Averaged!')\n",
        "                stored_loss = val_loss2\n",
        "\n",
        "            for prm in model.parameters():\n",
        "                prm.data = tmp[prm].clone()\n",
        "\n",
        "        else:\n",
        "            val_loss = evaluate(val_data, eval_batch_size)\n",
        "            print('-' * 89)\n",
        "            print(' valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f} | valid bpc {:8.3f}'.format(\n",
        "              val_loss, math.exp(val_loss), val_loss / math.log(2)))\n",
        "            print('-' * 89)\n",
        "\n",
        "            if val_loss < stored_loss:\n",
        "                model_save(args.save)\n",
        "                print('Saving model (new best validation)')\n",
        "                stored_loss = val_loss\n",
        "\n",
        "            if args.optim == 'sgd' and 't0' not in optimizer.param_groups[0] and (len(best_val_loss)>args.nonmono and val_loss > min(best_val_loss[:-args.nonmono])):\n",
        "                print('Switching to ASGD')\n",
        "                optimizer = torch.optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n",
        "\n",
        "            if epoch in args.when:\n",
        "                print('Saving model before learning rate decreased')\n",
        "                model_save('{}.e{}'.format(args.save, epoch))\n",
        "                print('Dividing learning rate by 10')\n",
        "                optimizer.param_groups[0]['lr'] /= 10.\n",
        "\n",
        "            best_val_loss.append(val_loss)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "model_load(args.save)\n",
        "\n",
        "# Run on test data."
      ],
      "metadata": {
        "id": "3aVdQQiEXj_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2845cef1-bab6-4f03-9cdf-17ecd481f74c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1\n",
            "train_loss   6.57 | train_ppl   712.12 | train_bpc    9.476\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  6.55 | valid ppl   699.81 | valid bpc    9.451\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:2\n",
            "train_loss   6.56 | train_ppl   708.16 | train_bpc    9.468\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  6.55 | valid ppl   698.62 | valid bpc    9.448\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:3\n",
            "train_loss   6.56 | train_ppl   705.85 | train_bpc    9.463\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  6.54 | valid ppl   693.57 | valid bpc    9.438\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:4\n",
            "train_loss   6.53 | train_ppl   682.69 | train_bpc    9.415\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  6.49 | valid ppl   656.82 | valid bpc    9.359\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:5\n",
            "train_loss   6.45 | train_ppl   635.67 | train_bpc    9.312\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  6.39 | valid ppl   595.88 | valid bpc    9.219\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:6\n",
            "train_loss   6.31 | train_ppl   550.98 | train_bpc    9.106\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  6.24 | valid ppl   511.85 | valid bpc    9.000\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:7\n",
            "train_loss   6.20 | train_ppl   492.28 | train_bpc    8.943\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  6.16 | valid ppl   474.79 | valid bpc    8.891\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:8\n",
            "train_loss   6.14 | train_ppl   462.32 | train_bpc    8.853\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  6.11 | valid ppl   450.73 | valid bpc    8.816\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:9\n",
            "train_loss   6.08 | train_ppl   439.20 | train_bpc    8.779\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  6.07 | valid ppl   432.03 | valid bpc    8.755\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:10\n",
            "train_loss   6.03 | train_ppl   417.13 | train_bpc    8.704\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  6.04 | valid ppl   420.29 | valid bpc    8.715\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:11\n",
            "train_loss   5.99 | train_ppl   397.96 | train_bpc    8.636\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.98 | valid ppl   395.52 | valid bpc    8.628\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:12\n",
            "train_loss   5.94 | train_ppl   378.85 | train_bpc    8.565\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.93 | valid ppl   374.86 | valid bpc    8.550\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:13\n",
            "train_loss   5.88 | train_ppl   356.03 | train_bpc    8.476\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.85 | valid ppl   348.39 | valid bpc    8.445\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:14\n",
            "train_loss   5.80 | train_ppl   331.27 | train_bpc    8.372\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.77 | valid ppl   322.04 | valid bpc    8.331\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:15\n",
            "train_loss   5.73 | train_ppl   308.76 | train_bpc    8.270\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.72 | valid ppl   303.59 | valid bpc    8.246\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:16\n",
            "train_loss   5.68 | train_ppl   292.54 | train_bpc    8.192\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.69 | valid ppl   296.22 | valid bpc    8.211\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:17\n",
            "train_loss   5.64 | train_ppl   280.86 | train_bpc    8.134\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.65 | valid ppl   284.47 | valid bpc    8.152\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:18\n",
            "train_loss   5.60 | train_ppl   270.83 | train_bpc    8.081\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.62 | valid ppl   274.99 | valid bpc    8.103\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:19\n",
            "train_loss   5.57 | train_ppl   262.71 | train_bpc    8.037\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.60 | valid ppl   271.22 | valid bpc    8.083\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:20\n",
            "train_loss   5.54 | train_ppl   255.78 | train_bpc    7.999\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.55 | valid ppl   257.83 | valid bpc    8.010\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:21\n",
            "train_loss   5.51 | train_ppl   248.26 | train_bpc    7.956\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.53 | valid ppl   251.85 | valid bpc    7.976\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:22\n",
            "train_loss   5.50 | train_ppl   243.60 | train_bpc    7.928\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.56 | valid ppl   260.21 | valid bpc    8.024\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:23\n",
            "train_loss   5.47 | train_ppl   237.35 | train_bpc    7.891\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.51 | valid ppl   247.41 | valid bpc    7.951\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:24\n",
            "train_loss   5.45 | train_ppl   233.35 | train_bpc    7.866\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.49 | valid ppl   241.76 | valid bpc    7.917\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:25\n",
            "train_loss   5.44 | train_ppl   229.68 | train_bpc    7.843\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.48 | valid ppl   240.83 | valid bpc    7.912\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:26\n",
            "train_loss   5.42 | train_ppl   225.99 | train_bpc    7.820\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.64 | valid ppl   280.71 | valid bpc    8.133\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:27\n",
            "train_loss   5.41 | train_ppl   223.10 | train_bpc    7.802\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.44 | valid ppl   230.41 | valid bpc    7.848\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:28\n",
            "train_loss   5.39 | train_ppl   219.40 | train_bpc    7.777\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.42 | valid ppl   224.97 | valid bpc    7.814\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:29\n",
            "train_loss   5.38 | train_ppl   216.94 | train_bpc    7.761\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.44 | valid ppl   229.46 | valid bpc    7.842\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:30\n",
            "train_loss   5.37 | train_ppl   215.00 | train_bpc    7.748\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.42 | valid ppl   226.41 | valid bpc    7.823\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:31\n",
            "train_loss   5.36 | train_ppl   212.10 | train_bpc    7.729\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.41 | valid ppl   223.71 | valid bpc    7.806\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:32\n",
            "train_loss   5.35 | train_ppl   210.12 | train_bpc    7.715\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.40 | valid ppl   221.91 | valid bpc    7.794\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:33\n",
            "train_loss   5.34 | train_ppl   208.40 | train_bpc    7.703\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.38 | valid ppl   217.72 | valid bpc    7.766\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:34\n",
            "train_loss   5.33 | train_ppl   206.67 | train_bpc    7.691\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.38 | valid ppl   218.01 | valid bpc    7.768\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:35\n",
            "train_loss   5.32 | train_ppl   204.81 | train_bpc    7.678\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.55 | valid ppl   256.67 | valid bpc    8.004\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:36\n",
            "train_loss   5.32 | train_ppl   204.14 | train_bpc    7.673\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.35 | valid ppl   211.50 | valid bpc    7.725\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:37\n",
            "train_loss   5.31 | train_ppl   201.91 | train_bpc    7.658\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.40 | valid ppl   220.57 | valid bpc    7.785\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:38\n",
            "train_loss   5.30 | train_ppl   200.22 | train_bpc    7.645\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.50 | valid ppl   245.90 | valid bpc    7.942\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:39\n",
            "train_loss   5.30 | train_ppl   200.35 | train_bpc    7.646\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.35 | valid ppl   211.57 | valid bpc    7.725\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:40\n",
            "train_loss   5.29 | train_ppl   198.57 | train_bpc    7.634\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.35 | valid ppl   210.31 | valid bpc    7.716\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:41\n",
            "train_loss   5.28 | train_ppl   197.23 | train_bpc    7.624\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.34 | valid ppl   208.66 | valid bpc    7.705\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:42\n",
            "train_loss   5.28 | train_ppl   196.16 | train_bpc    7.616\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.33 | valid ppl   206.05 | valid bpc    7.687\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:43\n",
            "train_loss   5.27 | train_ppl   195.24 | train_bpc    7.609\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.33 | valid ppl   206.73 | valid bpc    7.692\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:44\n",
            "train_loss   5.27 | train_ppl   194.07 | train_bpc    7.600\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.32 | valid ppl   205.29 | valid bpc    7.682\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:45\n",
            "train_loss   5.26 | train_ppl   192.08 | train_bpc    7.586\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.33 | valid ppl   207.32 | valid bpc    7.696\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:46\n",
            "train_loss   5.26 | train_ppl   192.07 | train_bpc    7.585\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.32 | valid ppl   204.83 | valid bpc    7.678\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:47\n",
            "train_loss   5.25 | train_ppl   190.22 | train_bpc    7.572\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.30 | valid ppl   200.38 | valid bpc    7.647\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:48\n",
            "train_loss   5.24 | train_ppl   189.51 | train_bpc    7.566\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.30 | valid ppl   200.32 | valid bpc    7.646\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:49\n",
            "train_loss   5.25 | train_ppl   189.68 | train_bpc    7.567\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.32 | valid ppl   203.38 | valid bpc    7.668\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:50\n",
            "train_loss   5.24 | train_ppl   188.26 | train_bpc    7.557\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.34 | valid ppl   207.77 | valid bpc    7.699\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:51\n",
            "train_loss   5.24 | train_ppl   188.41 | train_bpc    7.558\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.55 | valid ppl   258.42 | valid bpc    8.014\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:52\n",
            "train_loss   5.23 | train_ppl   186.90 | train_bpc    7.546\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.27 | valid ppl   195.19 | valid bpc    7.609\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:53\n",
            "train_loss   5.23 | train_ppl   185.99 | train_bpc    7.539\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.36 | valid ppl   212.86 | valid bpc    7.734\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:54\n",
            "train_loss   5.22 | train_ppl   185.26 | train_bpc    7.533\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.28 | valid ppl   197.29 | valid bpc    7.624\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:55\n",
            "train_loss   5.22 | train_ppl   185.80 | train_bpc    7.538\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.48 | valid ppl   239.60 | valid bpc    7.905\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:56\n",
            "train_loss   5.22 | train_ppl   185.01 | train_bpc    7.531\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.27 | valid ppl   195.16 | valid bpc    7.608\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:57\n",
            "train_loss   5.21 | train_ppl   183.72 | train_bpc    7.521\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.29 | valid ppl   198.98 | valid bpc    7.636\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:58\n",
            "train_loss   5.21 | train_ppl   183.45 | train_bpc    7.519\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.29 | valid ppl   198.51 | valid bpc    7.633\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:59\n",
            "train_loss   5.21 | train_ppl   182.54 | train_bpc    7.512\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.28 | valid ppl   196.53 | valid bpc    7.619\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:60\n",
            "train_loss   5.20 | train_ppl   181.61 | train_bpc    7.505\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.29 | valid ppl   197.71 | valid bpc    7.627\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:61\n",
            "train_loss   5.20 | train_ppl   182.05 | train_bpc    7.508\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.26 | valid ppl   192.45 | valid bpc    7.588\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:62\n",
            "train_loss   5.19 | train_ppl   180.34 | train_bpc    7.495\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.32 | valid ppl   204.50 | valid bpc    7.676\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:63\n",
            "train_loss   5.20 | train_ppl   180.51 | train_bpc    7.496\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.26 | valid ppl   192.82 | valid bpc    7.591\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:64\n",
            "train_loss   5.19 | train_ppl   179.96 | train_bpc    7.492\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.28 | valid ppl   196.26 | valid bpc    7.617\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:65\n",
            "train_loss   5.19 | train_ppl   179.31 | train_bpc    7.486\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.27 | valid ppl   194.99 | valid bpc    7.607\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:66\n",
            "train_loss   5.19 | train_ppl   179.72 | train_bpc    7.490\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.29 | valid ppl   198.02 | valid bpc    7.630\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:67\n",
            "train_loss   5.18 | train_ppl   178.11 | train_bpc    7.477\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.29 | valid ppl   199.17 | valid bpc    7.638\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:68\n",
            "train_loss   5.19 | train_ppl   178.60 | train_bpc    7.481\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.26 | valid ppl   192.31 | valid bpc    7.587\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:69\n",
            "train_loss   5.18 | train_ppl   177.70 | train_bpc    7.473\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.25 | valid ppl   190.73 | valid bpc    7.575\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:70\n",
            "train_loss   5.18 | train_ppl   177.36 | train_bpc    7.471\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.24 | valid ppl   188.78 | valid bpc    7.561\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:71\n",
            "train_loss   5.18 | train_ppl   177.68 | train_bpc    7.473\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.25 | valid ppl   190.33 | valid bpc    7.572\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:72\n",
            "train_loss   5.18 | train_ppl   178.08 | train_bpc    7.476\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.26 | valid ppl   191.95 | valid bpc    7.585\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:73\n",
            "train_loss   5.17 | train_ppl   175.76 | train_bpc    7.457\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.23 | valid ppl   186.50 | valid bpc    7.543\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:74\n",
            "train_loss   5.17 | train_ppl   176.10 | train_bpc    7.460\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.23 | valid ppl   187.08 | valid bpc    7.548\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:75\n",
            "train_loss   5.17 | train_ppl   176.15 | train_bpc    7.461\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.24 | valid ppl   188.89 | valid bpc    7.561\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:76\n",
            "train_loss   5.17 | train_ppl   176.19 | train_bpc    7.461\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.22 | valid ppl   184.99 | valid bpc    7.531\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:77\n",
            "train_loss   5.17 | train_ppl   175.50 | train_bpc    7.455\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.22 | valid ppl   185.20 | valid bpc    7.533\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:78\n",
            "train_loss   5.17 | train_ppl   175.12 | train_bpc    7.452\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.22 | valid ppl   185.48 | valid bpc    7.535\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:79\n",
            "train_loss   5.16 | train_ppl   174.56 | train_bpc    7.448\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.23 | valid ppl   186.19 | valid bpc    7.541\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:80\n",
            "train_loss   5.16 | train_ppl   173.46 | train_bpc    7.438\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.22 | valid ppl   185.69 | valid bpc    7.537\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:81\n",
            "train_loss   5.16 | train_ppl   174.46 | train_bpc    7.447\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.23 | valid ppl   187.49 | valid bpc    7.551\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:82\n",
            "train_loss   5.16 | train_ppl   174.46 | train_bpc    7.447\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.24 | valid ppl   188.26 | valid bpc    7.557\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:83\n",
            "train_loss   5.15 | train_ppl   173.03 | train_bpc    7.435\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.23 | valid ppl   187.49 | valid bpc    7.551\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:84\n",
            "train_loss   5.16 | train_ppl   174.23 | train_bpc    7.445\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.24 | valid ppl   188.46 | valid bpc    7.558\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:85\n",
            "train_loss   5.15 | train_ppl   173.18 | train_bpc    7.436\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.24 | valid ppl   187.77 | valid bpc    7.553\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:86\n",
            "train_loss   5.15 | train_ppl   172.69 | train_bpc    7.432\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.24 | valid ppl   188.91 | valid bpc    7.562\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:87\n",
            "train_loss   5.16 | train_ppl   173.52 | train_bpc    7.439\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.24 | valid ppl   188.40 | valid bpc    7.558\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:88\n",
            "train_loss   5.15 | train_ppl   172.38 | train_bpc    7.429\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.24 | valid ppl   189.05 | valid bpc    7.563\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:89\n",
            "train_loss   5.15 | train_ppl   173.29 | train_bpc    7.437\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.24 | valid ppl   187.96 | valid bpc    7.554\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:90\n",
            "train_loss   5.16 | train_ppl   173.54 | train_bpc    7.439\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.21 | valid ppl   183.92 | valid bpc    7.523\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:91\n",
            "train_loss   5.15 | train_ppl   172.49 | train_bpc    7.430\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.21 | valid ppl   183.71 | valid bpc    7.521\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:92\n",
            "train_loss   5.15 | train_ppl   171.98 | train_bpc    7.426\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.26 | valid ppl   191.83 | valid bpc    7.584\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:93\n",
            "train_loss   5.15 | train_ppl   171.84 | train_bpc    7.425\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.22 | valid ppl   185.33 | valid bpc    7.534\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:94\n",
            "train_loss   5.14 | train_ppl   171.48 | train_bpc    7.422\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.25 | valid ppl   189.95 | valid bpc    7.569\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:95\n",
            "train_loss   5.15 | train_ppl   172.00 | train_bpc    7.426\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.32 | valid ppl   204.16 | valid bpc    7.674\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:96\n",
            "train_loss   5.15 | train_ppl   171.63 | train_bpc    7.423\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.21 | valid ppl   183.18 | valid bpc    7.517\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:97\n",
            "train_loss   5.14 | train_ppl   171.32 | train_bpc    7.421\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.20 | valid ppl   181.81 | valid bpc    7.506\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:98\n",
            "train_loss   5.14 | train_ppl   169.93 | train_bpc    7.409\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.21 | valid ppl   183.11 | valid bpc    7.517\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:99\n",
            "train_loss   5.14 | train_ppl   170.21 | train_bpc    7.411\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.21 | valid ppl   183.30 | valid bpc    7.518\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:100\n",
            "train_loss   5.14 | train_ppl   170.23 | train_bpc    7.411\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.23 | valid ppl   186.49 | valid bpc    7.543\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:101\n",
            "train_loss   5.14 | train_ppl   170.80 | train_bpc    7.416\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.20 | valid ppl   180.87 | valid bpc    7.499\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:102\n",
            "train_loss   5.14 | train_ppl   169.87 | train_bpc    7.408\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.21 | valid ppl   182.76 | valid bpc    7.514\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:103\n",
            "train_loss   5.14 | train_ppl   170.36 | train_bpc    7.412\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.25 | valid ppl   190.81 | valid bpc    7.576\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:104\n",
            "-----------------------------------------------------------------------------------------\n",
            "Exiting from training early\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbmOIwkUeDO3",
        "outputId": "e1d121e6-7be7-4691-b403-c86dcbd683ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}