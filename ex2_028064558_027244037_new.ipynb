{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AK18k/ex2/blob/main/ex2_028064558_027244037_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning - Ex #2\n",
        "Avi Keinan 028064558\n",
        "Ofer Ballin 027244037\n",
        "\n",
        "This notebook implements LSTM and GRU with and without dropouts"
      ],
      "metadata": {
        "id": "MtUKmt-upo9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext google.colab.data_table\n",
        "import pdb"
      ],
      "metadata": {
        "id": "sNmJlmS3Dmkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AK18k/ex2"
      ],
      "metadata": {
        "id": "fd4aj_pCh7VP",
        "outputId": "119c36d2-84d5-40f7-98e3-ed2871333129",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ex2'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 86 (delta 32), reused 38 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (86/86), 3.84 MiB | 7.26 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "DATA_PATH = '/content/ex2/data/ptb'\n",
        "PATH = '/content/ex2'\n",
        "os.chdir('/content/ex2')\n",
        "!ls"
      ],
      "metadata": {
        "id": "IRPe9rMApsmL",
        "outputId": "904d9cdf-1b7e-4f0e-8c0b-1c9cac393aa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16849233702737923.pt\t\t\t      LICENSE\n",
            "adabound.py\t\t\t\t      locked_dropout.py\n",
            "corpus.77afd5b9c5825ef44c47fcd18f4839dd.data  main.py\n",
            "data\t\t\t\t\t      model.py\n",
            "data.py\t\t\t\t\t      pointer.py\n",
            "embed_regularize.py\t\t\t      __pycache__\n",
            "ex2\t\t\t\t\t      README.md\n",
            "ex2_028064558_027244037_new.ipynb\t      splitcross.py\n",
            "finetune.py\t\t\t\t      utils.py\n",
            "generate.py\t\t\t\t      weight_drop.py\n",
            "getdata.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/ex2')"
      ],
      "metadata": {
        "id": "_BC5Z_88jAF6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from adabound import AdaBound\n",
        "\n",
        "import data\n",
        "import model\n",
        "\n",
        "from utils import batchify, get_batch, repackage_hidden"
      ],
      "metadata": {
        "id": "gcHclAz2n2R8"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='PyTorch PennTreeBank RNN/LSTM Language Model')\n",
        "parser.add_argument('--data', type=str, default='data/ptb/',\n",
        "                    help='location of the data corpus')\n",
        "parser.add_argument('--model', type=str, default='LSTM',\n",
        "                    help='type of recurrent net (LSTM, QRNN, GRU)')\n",
        "parser.add_argument('--emsize', type=int, default=1500,\n",
        "                    help='size of word embeddings')\n",
        "parser.add_argument('--nhid', type=int, default=200,\n",
        "                    help='number of hidden units per layer')\n",
        "parser.add_argument('--nlayers', type=int, default=2,\n",
        "                    help='number of layers')\n",
        "parser.add_argument('--lr', type=float, default=0.1,\n",
        "                    help='initial learning rate')\n",
        "# parser.add_argument('--clip', type=float, default=0.25,\n",
        "#                     help='gradient clipping')\n",
        "parser.add_argument('--epochs', type=int, default=200,\n",
        "                    help='upper epoch limit')\n",
        "parser.add_argument('--batch_size', type=int, default=20, metavar='N',\n",
        "                    help='batch size')\n",
        "parser.add_argument('--bptt', type=int, default=35,\n",
        "                    help='sequence length')\n",
        "parser.add_argument('--dropout', type=float, default=0,\n",
        "                    help='dropout applied to layers (0 = no dropout)')\n",
        "parser.add_argument('--dropouth', type=float, default=0,\n",
        "                    help='dropout for rnn layers (0 = no dropout)')\n",
        "parser.add_argument('--dropouti', type=float, default=0,\n",
        "                    help='dropout for input embedding layers (0 = no dropout)')\n",
        "parser.add_argument('--dropoute', type=float, default=0,\n",
        "                    help='dropout to remove words from embedding layer (0 = no dropout)')\n",
        "parser.add_argument('--wdrop', type=float, default=0,\n",
        "                    help='amount of weight dropout to apply to the RNN hidden to hidden matrix')\n",
        "parser.add_argument('--seed', type=int, default=1111,\n",
        "                    help='random seed')\n",
        "parser.add_argument('--nonmono', type=int, default=1111,\n",
        "                    help='random seed')\n",
        "parser.add_argument('--cuda', action='store_false',\n",
        "                    help='use CUDA')\n",
        "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
        "                    help='report interval')\n",
        "randomhash = ''.join(str(time.time()).split('.'))\n",
        "parser.add_argument('--save', type=str,  default=randomhash+'.pt',\n",
        "                    help='path to save the final model')\n",
        "parser.add_argument('--alpha', type=float, default=2,\n",
        "                    help='alpha L2 regularization on RNN activation (alpha = 0 means no regularization)')\n",
        "parser.add_argument('--beta', type=float, default=1,\n",
        "                    help='beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)')\n",
        "parser.add_argument('--wdecay', type=float, default=5e-4,\n",
        "                    help='weight decay applied to all weights')\n",
        "parser.add_argument('--resume', type=str,  default='',\n",
        "                    help='path of model to resume')\n",
        "parser.add_argument('--when', nargs=\"+\", type=int, default=[-1],\n",
        "                    help='When (which epochs) to divide the learning rate by 10 - accepts multiple')\n",
        "parser.add_argument('--optim', default='sgd', type=str, help='optimizer',\n",
        "                    choices=['sgd', 'adagrad', 'adam', 'amsgrad', 'adabound', 'amsbound'])\n",
        "parser.add_argument('--momentum', default=0.9, type=float, help='momentum term')\n",
        "parser.add_argument('--beta1', default=0.9, type=float, help='Adam coefficients beta_1')\n",
        "parser.add_argument('--beta2', default=0.999, type=float, help='Adam coefficients beta_2')\n",
        "parser.add_argument('--final_lr', default=0.1, type=float,\n",
        "                    help='final learning rate of AdaBound')\n",
        "parser.add_argument('--gamma', default=1e-3, type=float,)\n",
        "parser.add_argument('--ita',default=1e-2, type=float)\n",
        "parser.add_argument('--weight_decay', default=5e-4, type=float,\n",
        "                    help='weight decay for optimizers')\n",
        "#args = parser.parse_args()\n",
        "args, unknown = parser.parse_known_args() #change args assignment to accomodate Jupiter execution (instead of command line)\n",
        "args.tied = True\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "print(args.cuda)\n",
        "if torch.cuda.is_available():\n",
        "    print('cuda is available')\n",
        "    if not args.cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "    else:\n",
        "        torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DjfO_4nqY7k",
        "outputId": "efbcf96d-c72c-452d-b062-52602a49988b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "cuda is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "def model_save(fn):\n",
        "    with open(fn, 'wb') as f:\n",
        "        torch.save([model, criterion, optimizer], f)\n",
        "\n",
        "def model_load(fn):\n",
        "    global model, criterion, optimizer\n",
        "    with open(fn, 'rb') as f:\n",
        "        model, criterion, optimizer = torch.load(f)\n",
        "\n",
        "import os\n",
        "import hashlib\n",
        "fn = 'corpus.{}.data'.format(hashlib.md5(args.data.encode()).hexdigest())\n",
        "if os.path.exists(fn):\n",
        "    print('Loading cached dataset...')\n",
        "    corpus = torch.load(fn)\n",
        "else:\n",
        "    print('Producing dataset...')\n",
        "    corpus = data.Corpus(args.data)\n",
        "    torch.save(corpus, fn)\n",
        "\n",
        "eval_batch_size = 10\n",
        "\n",
        "test_batch_size = 1\n",
        "train_data = batchify(corpus.train, args.batch_size, args)\n",
        "val_data = batchify(corpus.valid, eval_batch_size, args)\n",
        "test_data = batchify(corpus.test, test_batch_size, args)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g08GWZBlXT-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e1f482-b3c8-40a4-8a59-12b34975f7bd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.shape)\n",
        "print(args.batch_size)\n",
        "print(train_data[0][2].shape)\n",
        "print(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjjSjsQpq5Ej",
        "outputId": "fa89135d-2431-4680-db8d-6bf551cd0bc5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([46479, 20])\n",
            "20\n",
            "torch.Size([])\n",
            "Namespace(data='data/ptb/', model='LSTM', emsize=1500, nhid=650, nlayers=2, lr=0.1, epochs=200, batch_size=20, bptt=35, dropout=0, dropouth=0, dropouti=0, dropoute=0, wdrop=0, seed=1111, nonmono=1111, cuda=True, log_interval=200, save='16849330738481638.pt', alpha=2, beta=1, wdecay=0.0005, resume='', when=[-1], optim='sgd', momentum=0.9, beta1=0.9, beta2=0.999, final_lr=0.1, gamma=0.001, ita=0.01, weight_decay=0.0005, tied=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "from splitcross import SplitCrossEntropyLoss\n",
        "criterion = None\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.dropouth, args.dropouti, args.dropoute, args.wdrop, args.tied)\n",
        "###\n",
        "if args.resume:\n",
        "    print('Resuming model ...')\n",
        "    model_load(args.resume)\n",
        "    optimizer.param_groups[0]['lr'] = args.lr\n",
        "    model.dropouti, model.dropouth, model.dropout, args.dropoute = args.dropouti, args.dropouth, args.dropout, args.dropoute\n",
        "    if args.wdrop:\n",
        "        from weight_drop import WeightDrop\n",
        "        for rnn in model.rnns:\n",
        "            if type(rnn) == WeightDrop: rnn.dropout = args.wdrop\n",
        "            elif rnn.zoneout > 0: rnn.zoneout = args.wdrop\n",
        "###\n",
        "if not criterion:\n",
        "    splits = []\n",
        "    if ntokens > 500000:\n",
        "        # One Billion\n",
        "        # This produces fairly even matrix mults for the buckets:\n",
        "        # 0: 11723136, 1: 10854630, 2: 11270961, 3: 11219422\n",
        "        splits = [4200, 35000, 180000]\n",
        "    elif ntokens > 75000:\n",
        "        # WikiText-103\n",
        "        splits = [2800, 20000, 76000]\n",
        "    print('Using', splits)\n",
        "    criterion = SplitCrossEntropyLoss(args.emsize, splits=splits, verbose=False)\n",
        "###\n",
        "if args.cuda:\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "###\n",
        "params = list(model.parameters()) + list(criterion.parameters())\n",
        "total_params = sum(x.size()[0] * x.size()[1] if len(x.size()) > 1 else x.size()[0] for x in params if x.size())\n",
        "print('Args:', args)\n",
        "print('Model total parameters:', total_params)"
      ],
      "metadata": {
        "id": "P-lv_fRwXbch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c5df654-a6a6-4356-b4b2-30e56e584055"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LSTM(1500, 650), LSTM(650, 1500)]\n",
            "Using []\n",
            "Args: Namespace(data='data/ptb/', model='LSTM', emsize=1500, nhid=650, nlayers=2, lr=0.1, epochs=200, batch_size=20, bptt=35, dropout=0, dropouth=0, dropouti=0, dropoute=0, wdrop=0, seed=1111, nonmono=1111, cuda=True, log_interval=200, save='16849330738481638.pt', alpha=2, beta=1, wdecay=0.0005, resume='', when=[-1], optim='sgd', momentum=0.9, beta1=0.9, beta2=0.999, final_lr=0.1, gamma=0.001, ita=0.01, weight_decay=0.0005, tied=True)\n",
            "Model total parameters: 33517200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CdX7JOJSnwdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "def evaluate(data_source, batch_size=10):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    if args.model == 'QRNN': model.reset()\n",
        "    total_loss = 0\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    for i in range(0, data_source.size(0) - 1, args.bptt):\n",
        "        data, targets = get_batch(data_source, i, args, evaluation=True)       \n",
        "        output, hidden = model(data, hidden)\n",
        "        total_loss += len(data) * criterion(model.decoder.weight, model.decoder.bias, output, targets).data\n",
        "        hidden = repackage_hidden(hidden)\n",
        "    return total_loss.item() / len(data_source)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    if args.model == 'QRNN': model.reset()\n",
        "    total_loss = 0\n",
        "    # start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(args.batch_size)\n",
        "    batch, i = 0, 0\n",
        "    while i < train_data.size(0) - 1 - 1:\n",
        "        # bptt = args.bptt if np.random.random() < 0.95 else args.bptt / 2. avi keinan\n",
        "        bptt = args.bptt        \n",
        "        # Prevent excessively small or negative sequence lengths\n",
        "        # seq_len = max(5, int(np.random.normal(bptt, 5))) avi keinan\n",
        "        seq_len = bptt\n",
        "        # There's a very small chance that it could select a very long sequence length resulting in OOM\n",
        "        # seq_len = min(seq_len, args.bptt + 10)\n",
        "\n",
        "        lr2 = optimizer.param_groups[0]['lr']\n",
        "        optimizer.param_groups[0]['lr'] = lr2 * seq_len / args.bptt\n",
        "        model.train()\n",
        "        data, targets = get_batch(train_data, i, args, seq_len=seq_len)\n",
        "\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output, hidden, rnn_hs, dropped_rnn_hs = model(data, hidden, return_h=True)\n",
        "        raw_loss = criterion(model.decoder.weight, model.decoder.bias, output, targets)\n",
        "\n",
        "        loss = raw_loss\n",
        "        # Activiation Regularization\n",
        "        if args.alpha: loss = loss + sum(args.alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n",
        "        # Temporal Activation Regularization (slowness)\n",
        "        if args.beta: loss = loss + sum(args.beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        # if args.clip: torch.nn.utils.clip_grad_norm_(params, args.clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += raw_loss.data\n",
        "        optimizer.param_groups[0]['lr'] = lr2\n",
        "        # if batch % args.log_interval == 0 and batch > 0:\n",
        "        # total_loss = 0\n",
        "            # start_time = time.time()\n",
        "        ###\n",
        "        batch += 1\n",
        "        i += seq_len\n",
        "\n",
        "    train_loss = total_loss.item() / batch\n",
        "    # elapsed = time.time() - start_time\n",
        "    print('train_loss  {:5.2f} | train_ppl {:8.2f} | train_bpc {:8.3f}'.format(\n",
        "        train_loss, math.exp(train_loss), train_loss / math.log(2)))\n",
        "# Loop over epochs.\n",
        "lr = args.lr\n",
        "best_val_loss = []\n",
        "stored_loss = 100000000\n",
        "\n",
        "def create_optimizer(args, model_params):\n",
        "    if args.optim == 'sgd':\n",
        "        return optim.SGD(model_params, args.lr, momentum=args.momentum,\n",
        "                         weight_decay=args.weight_decay)\n",
        "    elif args.optim == 'adagrad':\n",
        "        return optim.Adagrad(model_params, args.lr, weight_decay=args.weight_decay)\n",
        "    elif args.optim == 'adam':\n",
        "        return optim.Adam(model_params, args.lr, betas=(args.beta1, args.beta2),\n",
        "                          weight_decay=args.weight_decay)\n",
        "    elif args.optim == 'amsgrad':\n",
        "        return optim.Adam(model_params, args.lr, betas=(args.beta1, args.beta2),\n",
        "                          weight_decay=args.weight_decay, amsgrad=True)\n",
        "    elif args.optim == 'adabound':\n",
        "        return AdaBound(model_params, args.lr, betas=(args.beta1, args.beta2),\n",
        "                        final_lr=args.final_lr, gamma=args.gamma,\n",
        "                        weight_decay=args.weight_decay)\n",
        "    else:\n",
        "        assert args.optim == 'amsbound'\n",
        "        return AdaBound(model_params, args.lr, betas=(args.beta1, args.beta2),\n",
        "                        final_lr=args.final_lr, gamma=args.gamma,\n",
        "                        weight_decay=args.weight_decay, amsbound=True)\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    optimizer = create_optimizer(args, model.parameters())\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        print(\"epoch:\" + str(epoch))\n",
        "        train()\n",
        "        if 't0' in optimizer.param_groups[0]:\n",
        "            tmp = {}\n",
        "            for prm in model.parameters():\n",
        "                tmp[prm] = prm.data.clone()\n",
        "                prm.data = optimizer.state[prm]['ax'].clone()\n",
        "\n",
        "            test_loss = evaluate(test_data, test_batch_size)\n",
        "            print('test loss {:5.2f} | test ppl {:8.2f} | test bpc {:8.3f}'.format(\n",
        "                test_loss, math.exp(test_loss), test_loss / math.log(2)))\n",
        "            val_loss2 = evaluate(val_data)\n",
        "            print('-' * 89)\n",
        "            print(' valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f} | valid bpc {:8.3f}'.format(\n",
        "                    val_loss2, math.exp(val_loss2), val_loss2 / math.log(2)))\n",
        "            print('-' * 89)\n",
        "\n",
        "            if val_loss2 < stored_loss:\n",
        "                model_save(args.save)\n",
        "                print('Saving Averaged!')\n",
        "                stored_loss = val_loss2\n",
        "\n",
        "            for prm in model.parameters():\n",
        "                prm.data = tmp[prm].clone()\n",
        "\n",
        "        else:\n",
        "            val_loss = evaluate(val_data, eval_batch_size)\n",
        "            print('-' * 89)\n",
        "            print(' valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f} | valid bpc {:8.3f}'.format(\n",
        "              val_loss, math.exp(val_loss), val_loss / math.log(2)))\n",
        "            print('-' * 89)\n",
        "\n",
        "            if val_loss < stored_loss:\n",
        "                model_save(args.save)\n",
        "                print('Saving model (new best validation)')\n",
        "                stored_loss = val_loss\n",
        "\n",
        "            if args.optim == 'sgd' and 't0' not in optimizer.param_groups[0] and (len(best_val_loss)>args.nonmono and val_loss > min(best_val_loss[:-args.nonmono])):\n",
        "                print('Switching to ASGD')\n",
        "                optimizer = torch.optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n",
        "\n",
        "            if epoch in args.when:\n",
        "                print('Saving model before learning rate decreased')\n",
        "                model_save('{}.e{}'.format(args.save, epoch))\n",
        "                print('Dividing learning rate by 10')\n",
        "                optimizer.param_groups[0]['lr'] /= 10.\n",
        "\n",
        "            best_val_loss.append(val_loss)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "model_load(args.save)\n",
        "\n",
        "# Run on test data."
      ],
      "metadata": {
        "id": "3aVdQQiEXj_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4604cf84-2e17-41c7-e983-de0b208e9a08"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1\n",
            "train_loss   6.54 | train_ppl   690.96 | train_bpc    9.432\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  6.29 | valid ppl   541.30 | valid bpc    9.080\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:2\n",
            "train_loss   6.09 | train_ppl   442.21 | train_bpc    8.789\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.97 | valid ppl   391.47 | valid bpc    8.613\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:3\n",
            "train_loss   5.84 | train_ppl   344.99 | train_bpc    8.430\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.80 | valid ppl   329.57 | valid bpc    8.364\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:4\n",
            "train_loss   5.70 | train_ppl   300.24 | train_bpc    8.230\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.69 | valid ppl   295.63 | valid bpc    8.208\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:5\n",
            "train_loss   5.63 | train_ppl   277.59 | train_bpc    8.117\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.62 | valid ppl   275.09 | valid bpc    8.104\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:6\n",
            "train_loss   5.57 | train_ppl   262.77 | train_bpc    8.038\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.56 | valid ppl   259.37 | valid bpc    8.019\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:7\n",
            "train_loss   5.53 | train_ppl   252.31 | train_bpc    7.979\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.56 | valid ppl   260.69 | valid bpc    8.026\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:8\n",
            "train_loss   5.50 | train_ppl   244.29 | train_bpc    7.932\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.55 | valid ppl   257.13 | valid bpc    8.006\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:9\n",
            "train_loss   5.47 | train_ppl   238.31 | train_bpc    7.897\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.49 | valid ppl   242.46 | valid bpc    7.922\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:10\n",
            "train_loss   5.45 | train_ppl   232.97 | train_bpc    7.864\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.48 | valid ppl   240.14 | valid bpc    7.908\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:11\n",
            "train_loss   5.43 | train_ppl   228.54 | train_bpc    7.836\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.47 | valid ppl   236.80 | valid bpc    7.888\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:12\n",
            "train_loss   5.42 | train_ppl   224.87 | train_bpc    7.813\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.46 | valid ppl   236.23 | valid bpc    7.884\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:13\n",
            "train_loss   5.40 | train_ppl   221.22 | train_bpc    7.789\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.50 | valid ppl   244.41 | valid bpc    7.933\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:14\n",
            "train_loss   5.39 | train_ppl   218.28 | train_bpc    7.770\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.45 | valid ppl   233.49 | valid bpc    7.867\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:15\n",
            "train_loss   5.38 | train_ppl   216.03 | train_bpc    7.755\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.46 | valid ppl   236.07 | valid bpc    7.883\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:16\n",
            "train_loss   5.37 | train_ppl   214.19 | train_bpc    7.743\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.41 | valid ppl   224.51 | valid bpc    7.811\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:17\n",
            "train_loss   5.36 | train_ppl   212.02 | train_bpc    7.728\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.43 | valid ppl   228.33 | valid bpc    7.835\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:18\n",
            "train_loss   5.35 | train_ppl   210.78 | train_bpc    7.720\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.44 | valid ppl   231.24 | valid bpc    7.853\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:19\n",
            "train_loss   5.34 | train_ppl   209.17 | train_bpc    7.709\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.42 | valid ppl   225.84 | valid bpc    7.819\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:20\n",
            "train_loss   5.34 | train_ppl   208.08 | train_bpc    7.701\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.41 | valid ppl   224.41 | valid bpc    7.810\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:21\n",
            "train_loss   5.33 | train_ppl   207.07 | train_bpc    7.694\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.42 | valid ppl   224.84 | valid bpc    7.813\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:22\n",
            "train_loss   5.33 | train_ppl   205.96 | train_bpc    7.686\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.40 | valid ppl   221.94 | valid bpc    7.794\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:23\n",
            "train_loss   5.32 | train_ppl   205.16 | train_bpc    7.681\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.40 | valid ppl   220.38 | valid bpc    7.784\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:24\n",
            "train_loss   5.32 | train_ppl   204.59 | train_bpc    7.677\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.39 | valid ppl   218.40 | valid bpc    7.771\n",
            "-----------------------------------------------------------------------------------------\n",
            "Saving model (new best validation)\n",
            "epoch:25\n",
            "train_loss   5.32 | train_ppl   203.52 | train_bpc    7.669\n",
            "-----------------------------------------------------------------------------------------\n",
            " valid loss  5.40 | valid ppl   220.51 | valid bpc    7.785\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:26\n",
            "-----------------------------------------------------------------------------------------\n",
            "Exiting from training early\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JbmOIwkUeDO3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}